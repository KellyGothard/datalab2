{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PAPER_DATA_PATH = '~/Documents/GitHub/datalab2/smac/all_paper_data.xlsx'\n",
    "FIG_1G_OUT = 'figs/one-gram-timeseries/'\n",
    "FIG_2G_OUT = 'figs/two-gram-timeseries/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in Trigger Other sheet from all_paper_data.xlsx\n",
    "trigger_other = pd.read_excel(ALL_PAPER_DATA_PATH, sheet_name = 'Trigger Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab columns we want, clean up the missing values (NA and 0)\n",
    "by_laws = trigger_other[['t_q9','Trig_date']]\n",
    "by_laws = by_laws.fillna('NA')\n",
    "by_laws['t_q9'] = by_laws['t_q9'].replace(0, 'NA')\n",
    "\n",
    "# Remove punctuation and lower\n",
    "for index, row in by_laws.iterrows():\n",
    "    row['t_q9'] = row['t_q9'].translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "# Convert to list of documents for vectorization\n",
    "corpus = list(by_laws['t_q9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onegram Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count matrix of each one-gram per document, throw into a dataframe\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1), stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "by_laws_1g = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "community    3071\n",
       "strangers    2780\n",
       "allowed      2564\n",
       "sick         2087\n",
       "000          1493\n",
       "hand         1339\n",
       "washing      1204\n",
       "burial       1203\n",
       "bush         1169\n",
       "dead         1150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a gander at the common ones\n",
    "by_laws_1g.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a column thats a sum accross all rows - total # of one-grams ocurrences for that row\n",
    "by_laws_1g['total_1grams'] = np.array(by_laws_1g.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigger date column\n",
    "by_laws_1g['Trig_date'] = by_laws['Trig_date']\n",
    "by_laws_1g = by_laws_1g.groupby('Trig_date').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of words to plot timeseries for\n",
    "words = ['community','strangers','burial','gathering','hand','bush']\n",
    "\n",
    "# plots plots plots\n",
    "for word in words:\n",
    "    fig = go.Figure()\n",
    "    by_laws_1g[word+'_freq'] = by_laws_1g[word]/by_laws_1g['total_1grams']\n",
    "    fig.add_trace(go.Scatter(x=by_laws_1g['Trig_date'], y= by_laws_1g[word+'_freq'], name=word, opacity=1))\n",
    "    fig.update_layout(\n",
    "                  title_text='By-Laws: \\\"'+word+'\\\" as Percentage of One-grams',\n",
    "                  yaxis_range=[0,0.6],\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Percentage of One-grams\",\n",
    "                  plot_bgcolor = 'rgba(0,0,0,0)',showlegend=True,\n",
    "                  font=dict(family=\"Computer Modern\",color = \"#000000\", size=22))\n",
    "\n",
    "    fig.write_image(FIG_1G_OUT+word+'.pdf', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count matrix of each bi-gram per document, throw into a dataframe\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2), stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "by_laws_2g = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "strangers allowed    1174\n",
       "bush meat             867\n",
       "hand shaking          681\n",
       "allowed community     676\n",
       "sick person           643\n",
       "sick people           635\n",
       "public gathering      623\n",
       "eating bush           588\n",
       "washing dead          573\n",
       "500 000               513\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a gander at the common ones\n",
    "by_laws_2g.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a column thats a sum accross all rows - total # of one-grams ocurrences for that row\n",
    "by_laws_2g['total_2grams'] = np.array(by_laws_2g.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigger date column\n",
    "by_laws_2g['Trig_date'] = by_laws['Trig_date']\n",
    "by_laws_2g = by_laws_2g.groupby('Trig_date').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of bi-grams to plot timeseries for\n",
    "words = ['strangers allowed','bush meat','hand shaking','public gathering','eating bush','washing dead']\n",
    "\n",
    "# plots plots plots\n",
    "for word in words:\n",
    "    fig = go.Figure()\n",
    "    by_laws_2g[word+'_freq'] = by_laws_2g[word]/by_laws_2g['total_2grams']\n",
    "    fig.add_trace(go.Scatter(x=by_laws_2g['Trig_date'], y= by_laws_2g[word+'_freq'], name=word, opacity=1))\n",
    "    fig.update_layout(\n",
    "                  title_text='By-Laws: \\\"'+word+'\\\" as Percentage of Bi-grams',\n",
    "                  yaxis_range=[0,0.6],\n",
    "                  xaxis_title=\"Date\",\n",
    "                  yaxis_title=\"Percentage of Bi-grams\",\n",
    "                  plot_bgcolor = 'rgba(0,0,0,0)',showlegend=True,\n",
    "                  font=dict(family=\"Computer Modern\",color = \"#000000\", size=22))\n",
    "\n",
    "    fig.write_image(FIG_2G_OUT+word.replace(' ','')+'.pdf', width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
